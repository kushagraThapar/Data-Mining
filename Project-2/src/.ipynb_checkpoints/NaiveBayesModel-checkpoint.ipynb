{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time\n",
    "from sklearn.utils.extmath import density\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read data from Excel file and create data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readExcelFile(filename):\n",
    "    orig_excel_df_sh_1 = pd.read_excel(filename, sheetname=0)\n",
    "    orig_excel_df_sh_2 = pd.read_excel(filename, sheetname=1)\n",
    "\n",
    "    orig_excel_df = orig_excel_df_sh_1.copy()\n",
    "    orig_excel_df = orig_excel_df.append(orig_excel_df_sh_2)\n",
    "    return orig_excel_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extractReqData(orig_excel_df):\n",
    "    del orig_excel_df['date']\n",
    "    del orig_excel_df['time']\n",
    "    return orig_excel_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readFormatInputData (filename):\n",
    "    tweet_df = readExcelFile(filename)\n",
    "    del tweet_df['date']\n",
    "    del tweet_df['time']\n",
    "    \n",
    "    tweet_df['Class'] = tweet_df['Class'].astype('str').map(lambda x: x.lstrip(' ').rstrip(' '))\n",
    "    tweet_df['Class'] = tweet_df['Class'].astype('str').map(lambda x: x.replace('!!!!', ''))\n",
    "    \n",
    "    tweet_data_df = tweet_df.loc[tweet_df.Class=='1'].append(tweet_df.loc[tweet_df.Class=='-1'])\n",
    "\n",
    "    return tweet_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Anootated tweet                                         Unnamed: 3  \\\n",
      " 0              NaN    1: positive, -1: negative, 0: neutral, 2: mixed   \n",
      " 1   10:28:53-05:00  Kirkpatrick, who wore a baseball cap embroider...   \n",
      " 2   10:09:00-05:00  Question: If <e>Romney</e> and <e>Obama</e> ha...   \n",
      " 3   10:04:30-05:00  #<e>obama</e> debates that Cracker Ass Cracker...   \n",
      " 4   10:00:36-05:00  RT @davewiner Slate: Blame <e>Obama</e> for fo...   \n",
      " 5   09:50:08-05:00  @Hollivan @hereistheanswer  Youre missing the ...   \n",
      " 6   09:48:54-05:00  <e>Mitt Romney</e> made all of his money himse...   \n",
      " 7   10:00:16-05:00  I was raised as a Democrat  left the party yea...   \n",
      " 8   09:48:07-05:00  The <e>Obama camp</e> can't afford to lower ex...   \n",
      " 9   09:52:47-05:00  Tonight's debate has that \"Game 7\" feel! This ...   \n",
      " 10  10:12:50-05:00  <e>Obama</e> pot <a>policy</a> disappointing -...   \n",
      " 11  10:12:11-05:00  Not all of Hollywood has his back! RT @RedAler...   \n",
      " 12  10:06:14-05:00  @hblodget i'd be grateful for scoop from you g...   \n",
      " 13  09:49:38-05:00  <e>Obama</e> must, to a degree, hit <e>MR</e> ...   \n",
      " 14  09:22:47-05:00  <e>Obama</e>'s Expedient Speak fair in order t...   \n",
      " 15  10:02:09-05:00  I had a dream that i was smoking with <e>Obama...   \n",
      " 16  10:02:57-05:00  The Washington Times: The President’s Populari...   \n",
      " 17  10:09:31-05:00  <e>Obama</e>'s priorities: $30K for \"gaydar\" r...   \n",
      " 18  09:32:54-05:00  United states of Islam ? Does <e>Obama</e> rea...   \n",
      " 19  09:18:04-05:00  @FightForJobs :they created this debt now plam...   \n",
      " 20  09:11:10-05:00  @mittromney <e>Obama</e> has not kept ONE of h...   \n",
      " 21  09:40:05-05:00  <e>Obama</e> vs <e>Romney</e> pt.2 tonight. Ob...   \n",
      " 22  09:35:05-05:00  This sub is here talking shit about <e>Obama</e>.   \n",
      " 23  09:46:05-05:00                               <e>obama</e> ist tot   \n",
      " 24  09:45:36-05:00  @rnixonjr when you go against your own ideas t...   \n",
      " 25  09:58:40-05:00  I just don't see how <e>Obama</e> can recover ...   \n",
      " 26  10:15:20-05:00           Still my Idol Mr President <e>Obama</e>.   \n",
      " 27  10:25:03-05:00  @ArturasR Pretty creepy! No wonder <e>Obama</e...   \n",
      " 28  10:18:27-05:00  Just saw a truck with a NASA sticker and an <e...   \n",
      " 29  10:03:26-05:00  debate #2 tonight! I hope <e>Obama</e> brings ...   \n",
      "..              ...                                                ...   \n",
      "NaN             NaN                                                NaN   \n",
      "NaN             NaN                                                NaN   \n",
      "NaN             NaN                                                NaN   \n",
      "NaN             NaN                                                NaN   \n",
      "NaN             NaN                                                NaN   \n",
      "NaN             NaN                                                NaN   \n",
      "NaN             NaN                                                NaN   \n",
      "NaN             NaN                                                NaN   \n",
      "NaN             NaN                                                NaN   \n",
      "NaN             NaN                                                NaN   \n",
      "NaN             NaN                                                NaN   \n",
      "NaN             NaN                                                NaN   \n",
      "NaN             NaN                                                NaN   \n",
      "NaN             NaN                                                NaN   \n",
      "NaN             NaN                                                NaN   \n",
      "NaN             NaN                                                NaN   \n",
      "NaN             NaN                                                NaN   \n",
      "NaN             NaN                                                NaN   \n",
      "NaN             NaN                                                NaN   \n",
      "NaN             NaN                                                NaN   \n",
      "NaN             NaN                                                NaN   \n",
      "NaN             NaN                                                NaN   \n",
      "NaN             NaN                                                NaN   \n",
      "NaN             NaN                                                NaN   \n",
      "NaN             NaN                                                NaN   \n",
      "NaN             NaN                                                NaN   \n",
      "NaN             NaN                                                NaN   \n",
      "NaN             NaN                                                NaN   \n",
      "NaN             NaN                                                NaN   \n",
      "NaN             NaN                                                NaN   \n",
      "\n",
      "    Unnamed: 4  Unnamed: 5  date                 time  2012-10-16 00:00:00  \\\n",
      " 0       Class  Your class   NaN                  NaN                  NaN   \n",
      " 1           0         NaN   NaN  2012-10-16 00:00:00                  NaN   \n",
      " 2           2         NaN   NaN  2016-12-10 00:00:00                  NaN   \n",
      " 3           1         NaN   NaN  2012-10-16 00:00:00                  NaN   \n",
      " 4           2         NaN   NaN  2012-10-16 00:00:00                  NaN   \n",
      " 5           0         NaN   NaN  2012-10-16 00:00:00                  NaN   \n",
      " 6           2         NaN   NaN  2012-10-16 00:00:00                  NaN   \n",
      " 7          -1         NaN   NaN  2012-10-16 00:00:00                  NaN   \n",
      " 8           0         NaN   NaN  2012-10-16 00:00:00                  NaN   \n",
      " 9           2         NaN   NaN  2012-10-16 00:00:00                  NaN   \n",
      " 10         -1         NaN   NaN  2012-10-16 00:00:00                  NaN   \n",
      " 11         -1         NaN   NaN  2012-10-16 00:00:00                  NaN   \n",
      " 12          2         NaN   NaN  2012-10-16 00:00:00                  NaN   \n",
      " 13          2         NaN   NaN  2012-10-16 00:00:00                  NaN   \n",
      " 14          0         NaN   NaN  2012-10-16 00:00:00                  NaN   \n",
      " 15          0         NaN   NaN  2012-10-16 00:00:00                  NaN   \n",
      " 16         -1         NaN   NaN  2012-10-16 00:00:00                  NaN   \n",
      " 17         -1         NaN   NaN  2012-10-16 00:00:00                  NaN   \n",
      " 18         -1         NaN   NaN  2012-10-16 00:00:00                  NaN   \n",
      " 19         -1         NaN   NaN  2012-10-16 00:00:00                  NaN   \n",
      " 20         -1         NaN   NaN  2012-10-16 00:00:00                  NaN   \n",
      " 21          2         NaN   NaN  2012-10-16 00:00:00                  NaN   \n",
      " 22          0         NaN   NaN  2012-10-16 00:00:00                  NaN   \n",
      " 23          0         NaN   NaN  2012-10-16 00:00:00                  NaN   \n",
      " 24          0         NaN   NaN  2012-10-16 00:00:00                  NaN   \n",
      " 25          2         NaN   NaN  2012-10-16 00:00:00                  NaN   \n",
      " 26          1         NaN   NaN  2012-10-16 00:00:00                  NaN   \n",
      " 27          0         NaN   NaN  2012-10-16 00:00:00                  NaN   \n",
      " 28          0         NaN   NaN  2012-10-16 00:00:00                  NaN   \n",
      " 29          0         NaN   NaN  2012-10-16 00:00:00                  NaN   \n",
      "..         ...         ...   ...                  ...                  ...   \n",
      "NaN        NaN         NaN   NaN                  NaN  2012-10-17 00:00:00   \n",
      "NaN        NaN         NaN   NaN                  NaN  2012-10-17 00:00:00   \n",
      "NaN        NaN         NaN   NaN                  NaN  2012-10-17 00:00:00   \n",
      "NaN        NaN         NaN   NaN                  NaN  2012-10-17 00:00:00   \n",
      "NaN        NaN         NaN   NaN                  NaN  2012-10-17 00:00:00   \n",
      "NaN        NaN         NaN   NaN                  NaN  2012-10-17 00:00:00   \n",
      "NaN        NaN         NaN   NaN                  NaN  2012-10-17 00:00:00   \n",
      "NaN        NaN         NaN   NaN                  NaN  2012-10-17 00:00:00   \n",
      "NaN        NaN         NaN   NaN                  NaN  2012-10-17 00:00:00   \n",
      "NaN        NaN         NaN   NaN                  NaN  2012-10-17 00:00:00   \n",
      "NaN        NaN         NaN   NaN                  NaN  2012-10-17 00:00:00   \n",
      "NaN        NaN         NaN   NaN                  NaN  2012-10-17 00:00:00   \n",
      "NaN        NaN         NaN   NaN                  NaN  2012-10-17 00:00:00   \n",
      "NaN        NaN         NaN   NaN                  NaN  2012-10-17 00:00:00   \n",
      "NaN        NaN         NaN   NaN                  NaN  2012-10-17 00:00:00   \n",
      "NaN        NaN         NaN   NaN                  NaN  2012-10-17 00:00:00   \n",
      "NaN        NaN         NaN   NaN                  NaN  2012-10-17 00:00:00   \n",
      "NaN        NaN         NaN   NaN                  NaN  2012-10-17 00:00:00   \n",
      "NaN        NaN         NaN   NaN                  NaN  2012-10-17 00:00:00   \n",
      "NaN        NaN         NaN   NaN                  NaN  2012-10-17 00:00:00   \n",
      "NaN        NaN         NaN   NaN                  NaN  2012-10-17 00:00:00   \n",
      "NaN        NaN         NaN   NaN                  NaN  2012-10-17 00:00:00   \n",
      "NaN        NaN         NaN   NaN                  NaN  2012-10-17 00:00:00   \n",
      "NaN        NaN         NaN   NaN                  NaN  2012-10-17 00:00:00   \n",
      "NaN        NaN         NaN   NaN                  NaN  2012-10-17 00:00:00   \n",
      "NaN        NaN         NaN   NaN                  NaN  2012-10-17 00:00:00   \n",
      "NaN        NaN         NaN   NaN                  NaN  2012-10-17 00:00:00   \n",
      "NaN        NaN         NaN   NaN                  NaN  2012-10-17 00:00:00   \n",
      "NaN        NaN         NaN   NaN                  NaN  2012-10-17 00:00:00   \n",
      "NaN        NaN         NaN   NaN                  NaN  2012-10-17 00:00:00   \n",
      "\n",
      "    09:38:08-05:00  \\\n",
      " 0             NaN   \n",
      " 1             NaN   \n",
      " 2             NaN   \n",
      " 3             NaN   \n",
      " 4             NaN   \n",
      " 5             NaN   \n",
      " 6             NaN   \n",
      " 7             NaN   \n",
      " 8             NaN   \n",
      " 9             NaN   \n",
      " 10            NaN   \n",
      " 11            NaN   \n",
      " 12            NaN   \n",
      " 13            NaN   \n",
      " 14            NaN   \n",
      " 15            NaN   \n",
      " 16            NaN   \n",
      " 17            NaN   \n",
      " 18            NaN   \n",
      " 19            NaN   \n",
      " 20            NaN   \n",
      " 21            NaN   \n",
      " 22            NaN   \n",
      " 23            NaN   \n",
      " 24            NaN   \n",
      " 25            NaN   \n",
      " 26            NaN   \n",
      " 27            NaN   \n",
      " 28            NaN   \n",
      " 29            NaN   \n",
      "..             ...   \n",
      "NaN    AM 10:28:11   \n",
      "NaN    AM 10:29:34   \n",
      "NaN    AM 10:32:12   \n",
      "NaN    AM 10:33:36   \n",
      "NaN    AM 10:35:12   \n",
      "NaN    AM 10:37:30   \n",
      "NaN    AM 10:37:59   \n",
      "NaN    AM 10:40:57   \n",
      "NaN    AM 10:43:32   \n",
      "NaN    AM 10:44:41   \n",
      "NaN    AM 10:46:03   \n",
      "NaN    AM 10:47:59   \n",
      "NaN    AM 10:49:03   \n",
      "NaN    AM 10:50:23   \n",
      "NaN    AM 10:51:53   \n",
      "NaN    AM 10:54:36   \n",
      "NaN    AM 10:56:57   \n",
      "NaN    AM 10:58:47   \n",
      "NaN     AM 11:0:58   \n",
      "NaN     AM 11:1:53   \n",
      "NaN     AM 11:4:47   \n",
      "NaN     AM 11:8:03   \n",
      "NaN     AM 11:9:02   \n",
      "NaN    AM 11:10:26   \n",
      "NaN    AM 11:12:47   \n",
      "NaN    AM 11:14:20   \n",
      "NaN    AM 11:15:53   \n",
      "NaN    AM 11:18:27   \n",
      "NaN    AM 11:20:04   \n",
      "NaN    AM 11:21:37   \n",
      "\n",
      "    Insidious!<e>Mitt Romney</e>'s Bain Helped Philip Morris Get U.S. High Schoolers <a>Hooked On Cigarettes</a> http://t.co/nMKuFcUq via @HuffPostPol  \\\n",
      " 0                                                 NaN                                                                                                   \n",
      " 1                                                 NaN                                                                                                   \n",
      " 2                                                 NaN                                                                                                   \n",
      " 3                                                 NaN                                                                                                   \n",
      " 4                                                 NaN                                                                                                   \n",
      " 5                                                 NaN                                                                                                   \n",
      " 6                                                 NaN                                                                                                   \n",
      " 7                                                 NaN                                                                                                   \n",
      " 8                                                 NaN                                                                                                   \n",
      " 9                                                 NaN                                                                                                   \n",
      " 10                                                NaN                                                                                                   \n",
      " 11                                                NaN                                                                                                   \n",
      " 12                                                NaN                                                                                                   \n",
      " 13                                                NaN                                                                                                   \n",
      " 14                                                NaN                                                                                                   \n",
      " 15                                                NaN                                                                                                   \n",
      " 16                                                NaN                                                                                                   \n",
      " 17                                                NaN                                                                                                   \n",
      " 18                                                NaN                                                                                                   \n",
      " 19                                                NaN                                                                                                   \n",
      " 20                                                NaN                                                                                                   \n",
      " 21                                                NaN                                                                                                   \n",
      " 22                                                NaN                                                                                                   \n",
      " 23                                                NaN                                                                                                   \n",
      " 24                                                NaN                                                                                                   \n",
      " 25                                                NaN                                                                                                   \n",
      " 26                                                NaN                                                                                                   \n",
      " 27                                                NaN                                                                                                   \n",
      " 28                                                NaN                                                                                                   \n",
      " 29                                                NaN                                                                                                   \n",
      "..                                                 ...                                                                                                   \n",
      "NaN  @Falsum So it's not so much the phrase itself\"...                                                                                                   \n",
      "NaN  Sounds like <e>Romney</e>is trying to run a bu...                                                                                                   \n",
      "NaN  Basically women should stay at home and people...                                                                                                   \n",
      "NaN  Either this is a really bad Halloween joke or ...                                                                                                   \n",
      "NaN  Idiot. \"\"\"\"@guardian: 'Binders full of women':...                                                                                                   \n",
      "NaN  @seanspicer Sean The comment that <e>Romney</e...                                                                                                   \n",
      "NaN       I hate <e>Mitt Romney</e>. #sorrynotsorry\"\"\"                                                                                                   \n",
      "NaN  @russwest44 #whynot vote for <e>obama</e>. <e>...                                                                                                   \n",
      "NaN  <e>Romney</e>Wins Debate on <a>Economy</a>: Th...                                                                                                   \n",
      "NaN  <e>obama</e> back in the fight after debate - ...                                                                                                   \n",
      "NaN  Mr. <e>Romney</e>...on <a>immigration</a> news...                                                                                                   \n",
      "NaN  Snoop Dogg's reasons for NOT voting for <e>Rom...                                                                                                   \n",
      "NaN  Shocked\"\" shocked... RT @joshtpm: Romney's 'Bi...                                                                                                   \n",
      "NaN  #Romney Deadass #TrueStory  http://t.co/ImHbcP...                                                                                                   \n",
      "NaN  @fredrickII he's shown he's by no means the pe...                                                                                                   \n",
      "NaN  MT @AlinskyDefeater: @MSNBC moron @hardball_ch...                                                                                                   \n",
      "NaN  I think the best argument <e>Romney</e>could m...                                                                                                   \n",
      "NaN  <e>Romney</e><a>campaign</a> finally admits he...                                                                                                   \n",
      "NaN  <e>Romney</e>aides take aim at CNN‚ Crowley af...                                                                                                   \n",
      "NaN  y'all are talking about <e>Romney</e>'s a liar...                                                                                                   \n",
      "NaN  Cheating <e>Romney</e>. The entitled elite who...                                                                                                   \n",
      "NaN  @SarahGo28 Challenge 13Million Democrats step ...                                                                                                   \n",
      "NaN  WASHINGTON (Reuters) ‚ With polls suggesting w...                                                                                                   \n",
      "NaN  <e>obama</e> Called Libya Attack Terrorism Lon...                                                                                                   \n",
      "NaN  <e>The President</e> looked ready for war last...                                                                                                   \n",
      "NaN  CNN's John King: After Two Debates\"\" <e>Romney...                                                                                                   \n",
      "NaN  el 59 por ciento de las mujeres blancas casada...                                                                                                   \n",
      "NaN  @FedericoArreola No vimos perder a <e>Romney</...                                                                                                   \n",
      "NaN  \"And they brought us a whole binder of women\"\"...                                                                                                   \n",
      "NaN                       @FoxNews <e>Romney</e>won\"\"\"                                                                                                   \n",
      "\n",
      "      -1  \n",
      " 0   NaN  \n",
      " 1   NaN  \n",
      " 2   NaN  \n",
      " 3   NaN  \n",
      " 4   NaN  \n",
      " 5   NaN  \n",
      " 6   NaN  \n",
      " 7   NaN  \n",
      " 8   NaN  \n",
      " 9   NaN  \n",
      " 10  NaN  \n",
      " 11  NaN  \n",
      " 12  NaN  \n",
      " 13  NaN  \n",
      " 14  NaN  \n",
      " 15  NaN  \n",
      " 16  NaN  \n",
      " 17  NaN  \n",
      " 18  NaN  \n",
      " 19  NaN  \n",
      " 20  NaN  \n",
      " 21  NaN  \n",
      " 22  NaN  \n",
      " 23  NaN  \n",
      " 24  NaN  \n",
      " 25  NaN  \n",
      " 26  NaN  \n",
      " 27  NaN  \n",
      " 28  NaN  \n",
      " 29  NaN  \n",
      "..   ...  \n",
      "NaN   -1  \n",
      "NaN   -1  \n",
      "NaN   -1  \n",
      "NaN   -1  \n",
      "NaN   -1  \n",
      "NaN   -1  \n",
      "NaN   -1  \n",
      "NaN    2  \n",
      "NaN    1  \n",
      "NaN    2  \n",
      "NaN   -1  \n",
      "NaN   -1  \n",
      "NaN   -1  \n",
      "NaN   -1  \n",
      "NaN    2  \n",
      "NaN   -1  \n",
      "NaN    2  \n",
      "NaN   -1  \n",
      "NaN    0  \n",
      "NaN    0  \n",
      "NaN   -1  \n",
      "NaN    0  \n",
      "NaN    1  \n",
      "NaN    2  \n",
      "NaN    2  \n",
      "NaN    1  \n",
      "NaN    0  \n",
      "NaN    2  \n",
      "NaN   -1  \n",
      "NaN    1  \n",
      "\n",
      "[14398 rows x 10 columns]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Class'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-11f8a966a804>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtweet_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreadFormatInputData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"training-Obama-Romney-tweets.xlsx\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-36-63c48d4181f7>\u001b[0m in \u001b[0;36mreadFormatInputData\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mtweet_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtweet_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Class'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtweet_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Class'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'str'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mtweet_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Class'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtweet_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Class'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'str'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'!!!!'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/kushagrathapar/anaconda/lib/python3.5/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1967\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1968\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1969\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1970\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1971\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/kushagrathapar/anaconda/lib/python3.5/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1974\u001b[0m         \u001b[0;31m# get column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1975\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1976\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1977\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1978\u001b[0m         \u001b[0;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/kushagrathapar/anaconda/lib/python3.5/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   1089\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1090\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1091\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1092\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1093\u001b[0m             \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/kushagrathapar/anaconda/lib/python3.5/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, item, fastpath)\u001b[0m\n\u001b[1;32m   3209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3210\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3211\u001b[0;31m                 \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3212\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3213\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/kushagrathapar/anaconda/lib/python3.5/site-packages/pandas/core/index.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   1757\u001b[0m                                  'backfill or nearest lookups')\n\u001b[1;32m   1758\u001b[0m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_values_from_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1759\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1760\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1761\u001b[0m         indexer = self.get_indexer([key], method=method,\n",
      "\u001b[0;32mpandas/index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas/index.c:3979)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas/index.c:3843)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/hashtable.pyx\u001b[0m in \u001b[0;36mpandas.hashtable.PyObjectHashTable.get_item (pandas/hashtable.c:12265)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/hashtable.pyx\u001b[0m in \u001b[0;36mpandas.hashtable.PyObjectHashTable.get_item (pandas/hashtable.c:12216)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Class'"
     ]
    }
   ],
   "source": [
    "tweet_df = readFormatInputData(\"training-Obama-Romney-tweets.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Slipt data into train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def splitTrainingData(df, train_data_prcnt=80):\n",
    "    msk = np.random.rand(len(df)) < train_data_prcnt/100\n",
    "    train = df[msk]\n",
    "    test = df[~msk]\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tweet_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-4f4d7236d9ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplitTrainingData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tweet_df' is not defined"
     ]
    }
   ],
   "source": [
    "train_df, test_df = splitTrainingData(tweet_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert Bag of Words model to sparce vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(stop_words='english')\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5, stop_words='english')\n",
    "hash_vectorizer = HashingVectorizer(stop_words='english', non_negative=True, n_features = 4000)\n",
    "tfidf_transformer = TfidfTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-00d9a8746dcb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_vect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Anootated tweet'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mX_test_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_vect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Anootated tweet'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# X_train_counts = hash_vectorizer.fit_transform(train_df['Anootated tweet'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# X_test_counts = hash_vectorizer.transform(test_df['Anootated tweet'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_df' is not defined"
     ]
    }
   ],
   "source": [
    "X_train_counts = count_vect.fit_transform(train_df['Anootated tweet'])\n",
    "X_test_counts = count_vect.transform(test_df['Anootated tweet'])\n",
    "\n",
    "# X_train_counts = hash_vectorizer.fit_transform(train_df['Anootated tweet'])\n",
    "# X_test_counts = hash_vectorizer.transform(test_df['Anootated tweet'])\n",
    "\n",
    "# X_train_counts = vectorizer.fit_transform(train_df['Anootated tweet'])\n",
    "# X_test_counts = vectorizer.transform(test_df['Anootated tweet'])\n",
    "\n",
    "X_train = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_test = tfidf_transformer.transform(X_test_counts)\n",
    "\n",
    "y_train = train_df['Class']\n",
    "y_test = test_df['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trim(s):\n",
    "    \"\"\"Trim string to fit on terminal (assuming 80-column display)\"\"\"\n",
    "    return s if len(s) <= 80 else s[:77] + \"...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def benchmark(clf):\n",
    "    print('_' * 80)\n",
    "    print(\"Training: \")\n",
    "    print(clf)\n",
    "    t0 = time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    train_time = time() - t0\n",
    "    print(\"train time: %0.3fs\" % train_time)\n",
    "\n",
    "    t0 = time()\n",
    "    pred = clf.predict(X_test)\n",
    "    test_time = time() - t0\n",
    "    print(\"test time:  %0.3fs\" % test_time)\n",
    "\n",
    "    score = metrics.accuracy_score(y_test, pred)\n",
    "    print(\"accuracy:   %0.3f\" % score)\n",
    "\n",
    "    if hasattr(clf, 'coef_'):\n",
    "        print(\"dimensionality: %d\" % clf.coef_.shape[1])\n",
    "        print(\"density: %f\" % density(clf.coef_))\n",
    "\n",
    "#         if feature_names is not None:\n",
    "#             print(\"top 10 keywords per class:\")\n",
    "#             for i, category in enumerate(categories):\n",
    "#                 top10 = np.argsort(clf.coef_[i])[-10:]\n",
    "#                 print(trim(\"%s: %s\"\n",
    "#                       % (category, \" \".join(feature_names[top10]))))\n",
    "#         print()\n",
    "\n",
    "    print(\"classification report:\")\n",
    "    print(metrics.classification_report(y_test, pred,\n",
    "                                        target_names=categories))\n",
    "\n",
    "    print(\"confusion matrix:\")\n",
    "    print(metrics.confusion_matrix(y_test, pred))\n",
    "\n",
    "    print()\n",
    "    clf_descr = str(clf).split('(')[0]\n",
    "    return clf_descr, score, train_time, test_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Naive Bayes\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n",
      "train time: 0.048s\n",
      "test time:  0.001s\n",
      "accuracy:   0.743\n",
      "dimensionality: 11567\n",
      "density: 1.000000\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.76      0.87      0.81       966\n",
      "          1       0.70      0.52      0.60       554\n",
      "\n",
      "avg / total       0.74      0.74      0.73      1520\n",
      "\n",
      "confusion matrix:\n",
      "[[839 127]\n",
      " [264 290]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "categories = np.unique(y_train.values)\n",
    "# Train sparse Naive Bayes classifiers\n",
    "print('=' * 80)\n",
    "print(\"Naive Bayes\")\n",
    "results.append(benchmark(MultinomialNB(alpha=.01)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-Nearest Neighbour Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "kNN\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=10, p=2,\n",
      "           weights='uniform')\n",
      "train time: 0.019s\n",
      "test time:  0.490s\n",
      "accuracy:   0.759\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.78      0.86      0.82       966\n",
      "          1       0.71      0.58      0.64       554\n",
      "\n",
      "avg / total       0.75      0.76      0.75      1520\n",
      "\n",
      "confusion matrix:\n",
      "[[833 133]\n",
      " [233 321]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train k-Nearest Neighbour classifiers\n",
    "print('=' * 80)\n",
    "print(\"kNN\")\n",
    "results.append(benchmark(KNeighborsClassifier(n_neighbors=10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Random Forest\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "train time: 12.541s\n",
      "test time:  0.276s\n",
      "accuracy:   0.762\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.79      0.86      0.82       966\n",
      "          1       0.71      0.59      0.64       554\n",
      "\n",
      "avg / total       0.76      0.76      0.76      1520\n",
      "\n",
      "confusion matrix:\n",
      "[[832 134]\n",
      " [227 327]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train Random Forest classifiers\n",
    "print('=' * 80)\n",
    "print(\"Random Forest\")\n",
    "results.append(benchmark(RandomForestClassifier(n_estimators=100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SGD Model\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
      "       learning_rate='optimal', loss='hinge', n_iter=50, n_jobs=1,\n",
      "       penalty='l1', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "train time: 0.119s\n",
      "test time:  0.000s\n",
      "accuracy:   0.761\n",
      "dimensionality: 11567\n",
      "density: 0.118354\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.79      0.85      0.82       966\n",
      "          1       0.70      0.61      0.65       554\n",
      "\n",
      "avg / total       0.76      0.76      0.76      1520\n",
      "\n",
      "confusion matrix:\n",
      "[[820 146]\n",
      " [217 337]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train SGD Classifier\n",
    "print('=' * 80)\n",
    "print(\"SGD Model\")\n",
    "results.append(benchmark(SGDClassifier(alpha=.0001, n_iter=50,\n",
    "                                           penalty='l1')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Linear SVM Model\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='l2', max_iter=1000, multi_class='ovr',\n",
      "     penalty='l1', random_state=None, tol=0.001, verbose=0)\n",
      "train time: 0.162s\n",
      "test time:  0.001s\n",
      "accuracy:   0.761\n",
      "dimensionality: 11567\n",
      "density: 0.174462\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.80      0.84      0.82       966\n",
      "          1       0.69      0.62      0.66       554\n",
      "\n",
      "avg / total       0.76      0.76      0.76      1520\n",
      "\n",
      "confusion matrix:\n",
      "[[812 154]\n",
      " [209 345]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/envs/py35/lib/python3.5/site-packages/sklearn/svm/classes.py:197: DeprecationWarning: loss='l2' has been deprecated in favor of loss='squared_hinge' as of 0.16. Backward compatibility for the loss='l2' will be removed in 1.0\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Train Linear SVM Classifier\n",
    "print('=' * 80)\n",
    "print(\"Linear SVM Model\")\n",
    "results.append(benchmark(LinearSVC(loss='l2', penalty='l1',\n",
    "                                            dual=False, tol=1e-3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
