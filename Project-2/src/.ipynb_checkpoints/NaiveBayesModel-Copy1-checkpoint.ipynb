{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "from time import time\n",
    "import string\n",
    "import itertools\n",
    "from html import unescape\n",
    "import preprocessor as p\n",
    "from sklearn.utils.extmath import density\n",
    "from sklearn import metrics\n",
    "from sklearn import cross_validation\n",
    "from sklearn import svm\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read data from Excel file and create data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def readExcelFile(filename):\n",
    "    orig_excel_df_sh_1 = pd.read_excel(filename, sheetname=0)\n",
    "    orig_excel_df_sh_2 = pd.read_excel(filename, sheetname=1)\n",
    "\n",
    "    orig_excel_df = orig_excel_df_sh_1.copy()\n",
    "    orig_excel_df = orig_excel_df.append(orig_excel_df_sh_2)\n",
    "    return orig_excel_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extractReqData(orig_excel_df):\n",
    "    del orig_excel_df['date']\n",
    "    del orig_excel_df['time']\n",
    "    return orig_excel_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def readFormatInputData (filename):\n",
    "    tweet_df = readExcelFile(filename)\n",
    "    del tweet_df['date']\n",
    "    del tweet_df['time']\n",
    "    \n",
    "    tweet_df['Class'] = tweet_df['Class'].astype('str').map(lambda x: x.lstrip(' ').rstrip(' '))\n",
    "    tweet_df['Class'] = tweet_df['Class'].astype('str').map(lambda x: x.replace('!!!!', ''))\n",
    "    \n",
    "    tweet_data_df = tweet_df.loc[tweet_df.Class=='1'].append(tweet_df.loc[tweet_df.Class=='-1']).append(tweet_df.loc[tweet_df.Class=='0'])\n",
    "    \n",
    "    tweet_data_df.dropna(inplace = True)\n",
    "\n",
    "    return tweet_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def splitTrainingData(df, train_data_prcnt):\n",
    "    msk = np.random.rand(len(df)) < train_data_prcnt/100\n",
    "    train = df[msk]\n",
    "    test = df[~msk]\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweet_df = readFormatInputData(\"training-Obama-Romney-tweets.xlsx\")\n",
    "exclude = set(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1 : 4861\n",
      " 1 : 2754\n",
      " 0 : 3657\n"
     ]
    }
   ],
   "source": [
    "print(\"-1 : \" + str(len(tweet_df.loc[tweet_df.Class=='-1'])))\n",
    "print(\" 1 : \" + str(len(tweet_df.loc[tweet_df.Class=='1'])))\n",
    "print(\" 0 : \" + str(len(tweet_df.loc[tweet_df.Class=='0'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweet_df = readExcelFile(\"training-Obama-Romney-tweets.xlsx\")\n",
    "del tweet_df['date']\n",
    "del tweet_df['time']\n",
    "\n",
    "tweet_df['Class'] = tweet_df['Class'].astype('str').map(lambda x: x.lstrip(' ').rstrip(' '))\n",
    "tweet_df['Class'] = tweet_df['Class'].astype('str').map(lambda x: x.replace('!!!!', ''))\n",
    "\n",
    "tweet_data_df = tweet_df.loc[tweet_df.Class=='-1']\n",
    "\n",
    "split1_df, split2_df = splitTrainingData(tweet_data_df, (len(tweet_df.loc[tweet_df.Class=='1']) / len(tweet_df.loc[tweet_df.Class=='-1'])) * 100)\n",
    "tweet_data_df = split1_df.copy()\n",
    "\n",
    "tweet_data_df = tweet_data_df.append(tweet_df.loc[tweet_df.Class=='1'])\n",
    "\n",
    "tweet_zero_df = tweet_df.loc[tweet_df.Class=='0']\n",
    "split1_df, split2_df = splitTrainingData(tweet_zero_df, (len(tweet_df.loc[tweet_df.Class=='1']) / len(tweet_df.loc[tweet_df.Class=='0'])) * 100)\n",
    "tweet_data_df = pd.concat([tweet_data_df, split1_df])\n",
    "\n",
    "tweet_data_df.dropna(inplace = True)\n",
    "\n",
    "tweet_df = tweet_data_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cleanTweetText(text):\n",
    "    \n",
    "    if not pd.isnull(text):\n",
    "        # Remove html escape characters and replace with their meaning\n",
    "        text = unescape(text)\n",
    "        # Decode tweet to utf-8 format\n",
    "\n",
    "        text = text.encode(\"ascii\", \"ignore\").decode(\"utf8\")\n",
    "\n",
    "        # Clean data using tweet preprocessor and convert to lower case\n",
    "        text = str.lower(p.clean(text))\n",
    "\n",
    "        # Remove characters\n",
    "        text = text.replace(\"<e>\", \"\")\n",
    "        text = text.replace(\"</e>\", \"\")\n",
    "        text = text.replace(\"<a>\", \"\")\n",
    "        text = text.replace(\"</a>\", \"\")\n",
    "\n",
    "        # Remove multiple repetition of a character in word\n",
    "        text = ''.join(''.join(s)[:2] for _, s in itertools.groupby(text))\n",
    "\n",
    "        # Remove punctuation\n",
    "        text = ''.join(ch for ch in text if ch not in exclude)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweet_df['Anootated tweet'] = tweet_df['Anootated tweet'].map(cleanTweetText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1 : 2772\n",
      " 1 : 2754\n",
      " 0 : 2785\n"
     ]
    }
   ],
   "source": [
    "print(\"-1 : \" + str(len(tweet_df.loc[tweet_df.Class=='-1'])))\n",
    "print(\" 1 : \" + str(len(tweet_df.loc[tweet_df.Class=='1'])))\n",
    "print(\" 0 : \" + str(len(tweet_df.loc[tweet_df.Class=='0'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Slipt data into train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweet_random_df = tweet_df.copy()\n",
    "#tweet_random_df = pd.DataFrame.drop_duplicates(tweet_random_df)\n",
    "\n",
    "for i in range(0, 500):\n",
    "    split1_df, split2_df = splitTrainingData(tweet_random_df, 50)\n",
    "    tweet_random_df = pd.concat([split1_df, split2_df])\n",
    "\n",
    "train_df, test_df = splitTrainingData(tweet_random_df, 75)\n",
    "#set(train_df['Anootated tweet']).intersection(test_df['Anootated tweet'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert Bag of Words model to sparce vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(stop_words='english', max_features=500 )\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5, stop_words='english', max_features=500)\n",
    "hash_vectorizer = HashingVectorizer(stop_words='english', non_negative=True, n_features = 2000)\n",
    "tfidf_transformer = TfidfTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_counts = count_vect.fit_transform(train_df['Anootated tweet'])\n",
    "X_test_counts = count_vect.transform(test_df['Anootated tweet'])\n",
    "\n",
    "# X_train_counts = hash_vectorizer.fit_transform(train_df['Anootated tweet'])\n",
    "# X_test_counts = hash_vectorizer.transform(test_df['Anootated tweet'])\n",
    "\n",
    "# X_train_counts = vectorizer.fit_transform(train_df['Anootated tweet'])\n",
    "# X_test_counts = vectorizer.transform(test_df['Anootated tweet'])\n",
    "\n",
    "X_train = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_test = tfidf_transformer.transform(X_test_counts)\n",
    "\n",
    "y_train = train_df['Class']\n",
    "y_test = test_df['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def trim(s):\n",
    "    \"\"\"Trim string to fit on terminal (assuming 80-column display)\"\"\"\n",
    "    return s if len(s) <= 80 else s[:77] + \"...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Classification function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def benchmark(clf):\n",
    "    print('_' * 80)\n",
    "    print(\"Training: \")\n",
    "    print(clf)\n",
    "    t0 = time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    train_time = time() - t0\n",
    "    print(\"train time: %0.3fs\" % train_time)\n",
    "\n",
    "    t0 = time()\n",
    "    pred = clf.predict(X_test)\n",
    "    test_time = time() - t0\n",
    "    print(\"test time:  %0.3fs\" % test_time)\n",
    "\n",
    "    score = metrics.accuracy_score(y_test, pred)\n",
    "    print(\"accuracy:   %0.3f\" % score)\n",
    "\n",
    "    if hasattr(clf, 'coef_'):\n",
    "        print(\"dimensionality: %d\" % clf.coef_.shape[1])\n",
    "        print(\"density: %f\" % density(clf.coef_))\n",
    "\n",
    "#         if feature_names is not None:\n",
    "#             print(\"top 10 keywords per class:\")\n",
    "#             for i, category in enumerate(categories):\n",
    "#                 top10 = np.argsort(clf.coef_[i])[-10:]\n",
    "#                 print(trim(\"%s: %s\"\n",
    "#                       % (category, \" \".join(feature_names[top10]))))\n",
    "#         print()\n",
    "\n",
    "    print(\"classification report:\")\n",
    "    print(metrics.classification_report(y_test, pred,\n",
    "                                        target_names=categories))\n",
    "\n",
    "    print(\"confusion matrix:\")\n",
    "    print(metrics.confusion_matrix(y_test, pred))\n",
    "\n",
    "    print()\n",
    "    clf_descr = str(clf).split('(')[0]\n",
    "    return clf_descr, score, train_time, test_time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Naive Bayes\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n",
      "train time: 0.022s\n",
      "test time:  0.000s\n",
      "accuracy:   0.531\n",
      "dimensionality: 500\n",
      "density: 1.000000\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.48      0.56      0.52       668\n",
      "          0       0.50      0.41      0.45       699\n",
      "          1       0.60      0.63      0.61       708\n",
      "\n",
      "avg / total       0.53      0.53      0.53      2075\n",
      "\n",
      "confusion matrix:\n",
      "[[373 164 131]\n",
      " [250 285 164]\n",
      " [148 116 444]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "categories = np.unique(y_train.values)\n",
    "# Train sparse Naive Bayes classifiers\n",
    "print('=' * 80)\n",
    "print(\"Naive Bayes\")\n",
    "results.append(benchmark(MultinomialNB(alpha=.01)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-Nearest Neighbour Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "kNN\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=-1, n_neighbors=10, p=2,\n",
      "           weights='uniform')\n",
      "train time: 0.008s\n",
      "test time:  1.384s\n",
      "accuracy:   0.455\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.48      0.28      0.35       668\n",
      "          0       0.40      0.65      0.49       699\n",
      "          1       0.56      0.44      0.49       708\n",
      "\n",
      "avg / total       0.48      0.45      0.45      2075\n",
      "\n",
      "confusion matrix:\n",
      "[[184 370 114]\n",
      " [119 451 129]\n",
      " [ 82 317 309]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train k-Nearest Neighbour classifiers\n",
    "print('=' * 80)\n",
    "print(\"kNN\")\n",
    "results.append(benchmark(KNeighborsClassifier(n_neighbors=10, n_jobs = -1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Random Forest\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "train time: 3.527s\n",
      "test time:  0.104s\n",
      "accuracy:   0.529\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.51      0.56      0.54       668\n",
      "          0       0.48      0.48      0.48       699\n",
      "          1       0.60      0.55      0.57       708\n",
      "\n",
      "avg / total       0.53      0.53      0.53      2075\n",
      "\n",
      "confusion matrix:\n",
      "[[373 176 119]\n",
      " [219 339 141]\n",
      " [133 189 386]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train Random Forest classifiers\n",
    "print('=' * 80)\n",
    "print(\"Random Forest\")\n",
    "results.append(benchmark(RandomForestClassifier(n_estimators=100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SGD Model\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
      "       learning_rate='optimal', loss='hinge', n_iter=50, n_jobs=1,\n",
      "       penalty='l1', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "train time: 0.170s\n",
      "test time:  0.000s\n",
      "accuracy:   0.526\n",
      "dimensionality: 500\n",
      "density: 0.730000\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.50      0.49      0.50       668\n",
      "          0       0.50      0.46      0.48       699\n",
      "          1       0.57      0.62      0.59       708\n",
      "\n",
      "avg / total       0.52      0.53      0.52      2075\n",
      "\n",
      "confusion matrix:\n",
      "[[329 183 156]\n",
      " [196 324 179]\n",
      " [127 142 439]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train SGD Classifier\n",
    "print('=' * 80)\n",
    "print(\"SGD Model\")\n",
    "results.append(benchmark(SGDClassifier(alpha=.0001, n_iter=50,\n",
    "                                           penalty='l1')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Linear SVM Model\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='l2', max_iter=1000, multi_class='ovr',\n",
      "     penalty='l2', random_state=None, tol=0.001, verbose=0)\n",
      "train time: 0.063s\n",
      "test time:  0.000s\n",
      "accuracy:   0.514\n",
      "dimensionality: 500\n",
      "density: 1.000000\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.48      0.52      0.50       668\n",
      "          0       0.48      0.46      0.47       699\n",
      "          1       0.59      0.56      0.58       708\n",
      "\n",
      "avg / total       0.52      0.51      0.51      2075\n",
      "\n",
      "confusion matrix:\n",
      "[[348 194 126]\n",
      " [231 324 144]\n",
      " [151 162 395]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kushagrathapar/anaconda/lib/python3.5/site-packages/sklearn/svm/classes.py:197: DeprecationWarning: loss='l2' has been deprecated in favor of loss='squared_hinge' as of 0.16. Backward compatibility for the loss='l2' will be removed in 1.0\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Train Linear SVM Classifier\n",
    "print('=' * 80)\n",
    "print(\"Linear SVM Model\")\n",
    "results.append(benchmark(LinearSVC(loss='l2', penalty='l2',\n",
    "                                            dual=False, tol=1e-3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Cross Validation (10 fold) on Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kushagrathapar/anaconda/lib/python3.5/site-packages/sklearn/svm/classes.py:197: DeprecationWarning: loss='l2' has been deprecated in favor of loss='squared_hinge' as of 0.16. Backward compatibility for the loss='l2' will be removed in 1.0\n",
      "  DeprecationWarning)\n",
      "/Users/kushagrathapar/anaconda/lib/python3.5/site-packages/sklearn/svm/classes.py:197: DeprecationWarning: loss='l2' has been deprecated in favor of loss='squared_hinge' as of 0.16. Backward compatibility for the loss='l2' will be removed in 1.0\n",
      "  DeprecationWarning)\n",
      "/Users/kushagrathapar/anaconda/lib/python3.5/site-packages/sklearn/svm/classes.py:197: DeprecationWarning: loss='l2' has been deprecated in favor of loss='squared_hinge' as of 0.16. Backward compatibility for the loss='l2' will be removed in 1.0\n",
      "  DeprecationWarning)\n",
      "/Users/kushagrathapar/anaconda/lib/python3.5/site-packages/sklearn/svm/classes.py:197: DeprecationWarning: loss='l2' has been deprecated in favor of loss='squared_hinge' as of 0.16. Backward compatibility for the loss='l2' will be removed in 1.0\n",
      "  DeprecationWarning)\n",
      "/Users/kushagrathapar/anaconda/lib/python3.5/site-packages/sklearn/svm/classes.py:197: DeprecationWarning: loss='l2' has been deprecated in favor of loss='squared_hinge' as of 0.16. Backward compatibility for the loss='l2' will be removed in 1.0\n",
      "  DeprecationWarning)\n",
      "/Users/kushagrathapar/anaconda/lib/python3.5/site-packages/sklearn/svm/classes.py:197: DeprecationWarning: loss='l2' has been deprecated in favor of loss='squared_hinge' as of 0.16. Backward compatibility for the loss='l2' will be removed in 1.0\n",
      "  DeprecationWarning)\n",
      "/Users/kushagrathapar/anaconda/lib/python3.5/site-packages/sklearn/svm/classes.py:197: DeprecationWarning: loss='l2' has been deprecated in favor of loss='squared_hinge' as of 0.16. Backward compatibility for the loss='l2' will be removed in 1.0\n",
      "  DeprecationWarning)\n",
      "/Users/kushagrathapar/anaconda/lib/python3.5/site-packages/sklearn/svm/classes.py:197: DeprecationWarning: loss='l2' has been deprecated in favor of loss='squared_hinge' as of 0.16. Backward compatibility for the loss='l2' will be removed in 1.0\n",
      "  DeprecationWarning)\n",
      "/Users/kushagrathapar/anaconda/lib/python3.5/site-packages/sklearn/svm/classes.py:197: DeprecationWarning: loss='l2' has been deprecated in favor of loss='squared_hinge' as of 0.16. Backward compatibility for the loss='l2' will be removed in 1.0\n",
      "  DeprecationWarning)\n",
      "/Users/kushagrathapar/anaconda/lib/python3.5/site-packages/sklearn/svm/classes.py:197: DeprecationWarning: loss='l2' has been deprecated in favor of loss='squared_hinge' as of 0.16. Backward compatibility for the loss='l2' will be removed in 1.0\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.51491109  0.50926273  0.5255323   0.53831795  0.53195528  0.52820111\n",
      "  0.51032541  0.51665605  0.50321802  0.54050534]\n",
      "Accuracy: 0.52 (+/- 0.02)\n"
     ]
    }
   ],
   "source": [
    "# print('The scikit-learn version is {}.'.format(sklearn.__version__))\n",
    "\n",
    "tweet_random_df = tweet_df.copy()\n",
    "len(tweet_random_df)\n",
    "for i in range(0, 100):\n",
    "    split1_df, split2_df = splitTrainingData(tweet_random_df, 50)\n",
    "    tweet_random_df = pd.concat([split1_df, split2_df])\n",
    "    \n",
    "X_kfcv_counts = count_vect.fit_transform(tweet_random_df['Anootated tweet'])\n",
    "X_kfcv = tfidf_transformer.fit_transform(X_kfcv_counts)\n",
    "Y_kfcv = tweet_random_df['Class']\n",
    "\n",
    "clf = LinearSVC(loss='l2', penalty='l2',dual=False, tol=1e-3)\n",
    "scores = cross_validation.cross_val_score(clf, X_kfcv, Y_kfcv, cv=10, scoring='f1_weighted')\n",
    "print(scores)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Cross Validation (10 fold) on Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.51932427  0.52600686  0.521578    0.53475984  0.52127401  0.51726834\n",
      "  0.50077517  0.51779986  0.47561899  0.55112091]\n",
      "Accuracy: 0.52 (+/- 0.04)\n"
     ]
    }
   ],
   "source": [
    "clf = MultinomialNB(alpha = 0.01)\n",
    "scores = cross_validation.cross_val_score(clf, X_kfcv, Y_kfcv, cv=10, scoring='f1_weighted')\n",
    "print(scores)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Cross Validation (10 fold) on Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.52671174  0.53067783  0.52927018  0.51947334  0.5515291   0.51810845\n",
      "  0.51657814  0.52599158  0.5071992   0.53518984]\n",
      "Accuracy: 0.53 (+/- 0.02)\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "scores = cross_validation.cross_val_score(clf, X_kfcv, Y_kfcv, cv=10, scoring='f1_weighted')\n",
    "print(scores)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation (10 fold) on Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.51130847  0.5083669   0.51208124  0.52683791  0.53430661  0.51757969\n",
      "  0.50372526  0.49712372  0.44395876  0.52941589]\n",
      "Accuracy: 0.51 (+/- 0.05)\n"
     ]
    }
   ],
   "source": [
    "clf = SGDClassifier(alpha=.0001, n_iter=50, penalty='l1')\n",
    "scores = cross_validation.cross_val_score(clf, X_kfcv, Y_kfcv, cv=10, scoring='f1_weighted')\n",
    "print(scores)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=10, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KNeighborsClassifier(n_neighbors=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
