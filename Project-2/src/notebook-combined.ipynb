{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "from time import time\n",
    "import string\n",
    "import itertools\n",
    "from html import unescape\n",
    "import preprocessor as p\n",
    "from sklearn.utils.extmath import density\n",
    "from sklearn import metrics\n",
    "from sklearn import cross_validation\n",
    "from sklearn import svm\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read data from Excel file and create data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def readExcelFile(filename):\n",
    "    orig_excel_df_sh_1 = pd.read_excel(filename, sheetname=0)\n",
    "    orig_excel_df_sh_2 = pd.read_excel(filename, sheetname=1)\n",
    "\n",
    "    orig_excel_df = orig_excel_df_sh_1.copy()\n",
    "    orig_excel_df = orig_excel_df.append(orig_excel_df_sh_2)\n",
    "    return orig_excel_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extractReqData(orig_excel_df):\n",
    "    del orig_excel_df['date']\n",
    "    del orig_excel_df['time']\n",
    "    return orig_excel_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def readFormatInputData (filename):\n",
    "    tweet_df = readExcelFile(filename)\n",
    "    del tweet_df['date']\n",
    "    del tweet_df['time']\n",
    "    \n",
    "    tweet_df['Class'] = tweet_df['Class'].astype('str').map(lambda x: x.lstrip(' ').rstrip(' '))\n",
    "    tweet_df['Class'] = tweet_df['Class'].astype('str').map(lambda x: x.replace('!!!!', ''))\n",
    "    \n",
    "    tweet_data_df = tweet_df.loc[tweet_df.Class=='1'].append(tweet_df.loc[tweet_df.Class=='-1']).append(tweet_df.loc[tweet_df.Class=='0'])\n",
    "    \n",
    "    tweet_data_df.dropna(inplace = True)\n",
    "\n",
    "    return tweet_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def splitTrainingData(df, train_data_prcnt):\n",
    "    msk = np.random.rand(len(df)) < train_data_prcnt/100\n",
    "    train = df[msk]\n",
    "    test = df[~msk]\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweet_df = readFormatInputData(\"training-Obama-Romney-tweets.xlsx\")\n",
    "exclude = set(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1 : 4861\n",
      " 1 : 2754\n",
      " 0 : 3657\n"
     ]
    }
   ],
   "source": [
    "print(\"-1 : \" + str(len(tweet_df.loc[tweet_df.Class=='-1'])))\n",
    "print(\" 1 : \" + str(len(tweet_df.loc[tweet_df.Class=='1'])))\n",
    "print(\" 0 : \" + str(len(tweet_df.loc[tweet_df.Class=='0'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweet_df = readExcelFile(\"training-Obama-Romney-tweets.xlsx\")\n",
    "del tweet_df['date']\n",
    "del tweet_df['time']\n",
    "\n",
    "tweet_df['Class'] = tweet_df['Class'].astype('str').map(lambda x: x.lstrip(' ').rstrip(' '))\n",
    "tweet_df['Class'] = tweet_df['Class'].astype('str').map(lambda x: x.replace('!!!!', ''))\n",
    "\n",
    "tweet_data_df = tweet_df.loc[tweet_df.Class=='-1']\n",
    "\n",
    "split1_df, split2_df = splitTrainingData(tweet_data_df, (len(tweet_df.loc[tweet_df.Class=='1']) / len(tweet_df.loc[tweet_df.Class=='-1'])) * 100)\n",
    "tweet_data_df = split1_df.copy()\n",
    "\n",
    "tweet_data_df = tweet_data_df.append(tweet_df.loc[tweet_df.Class=='1'])\n",
    "\n",
    "tweet_zero_df = tweet_df.loc[tweet_df.Class=='0']\n",
    "split1_df, split2_df = splitTrainingData(tweet_zero_df, (len(tweet_df.loc[tweet_df.Class=='1']) / len(tweet_df.loc[tweet_df.Class=='0'])) * 100)\n",
    "tweet_data_df = pd.concat([tweet_data_df, split1_df])\n",
    "\n",
    "tweet_data_df.dropna(inplace = True)\n",
    "\n",
    "tweet_df = tweet_data_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cleanTweetText(text):\n",
    "    \n",
    "    if not pd.isnull(text):\n",
    "        # Remove html escape characters and replace with their meaning\n",
    "        text = unescape(text)\n",
    "        # Decode tweet to utf-8 format\n",
    "\n",
    "        text = text.encode(\"ascii\", \"ignore\").decode(\"utf8\")\n",
    "\n",
    "        # Clean data using tweet preprocessor and convert to lower case\n",
    "        text = str.lower(p.clean(text))\n",
    "\n",
    "        # Remove characters\n",
    "        text = text.replace(\"<e>\", \"\")\n",
    "        text = text.replace(\"</e>\", \"\")\n",
    "        text = text.replace(\"<a>\", \"\")\n",
    "        text = text.replace(\"</a>\", \"\")\n",
    "\n",
    "        # Remove multiple repetition of a character in word\n",
    "        text = ''.join(''.join(s)[:2] for _, s in itertools.groupby(text))\n",
    "\n",
    "        # Remove punctuation\n",
    "        text = ''.join(ch for ch in text if ch not in exclude)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweet_df['Anootated tweet'] = tweet_df['Anootated tweet'].map(cleanTweetText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1 : 2750\n",
      " 1 : 2754\n",
      " 0 : 2776\n"
     ]
    }
   ],
   "source": [
    "print(\"-1 : \" + str(len(tweet_df.loc[tweet_df.Class=='-1'])))\n",
    "print(\" 1 : \" + str(len(tweet_df.loc[tweet_df.Class=='1'])))\n",
    "print(\" 0 : \" + str(len(tweet_df.loc[tweet_df.Class=='0'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Slipt data into train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweet_random_df = tweet_df.copy()\n",
    "#tweet_random_df = pd.DataFrame.drop_duplicates(tweet_random_df)\n",
    "\n",
    "for i in range(0, 500):\n",
    "    split1_df, split2_df = splitTrainingData(tweet_random_df, 50)\n",
    "    tweet_random_df = pd.concat([split1_df, split2_df])\n",
    "\n",
    "train_df, test_df = splitTrainingData(tweet_random_df, 75)\n",
    "#set(train_df['Anootated tweet']).intersection(test_df['Anootated tweet'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert Bag of Words model to sparce vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(stop_words='english', max_features=500 )\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5, stop_words='english', max_features=500)\n",
    "hash_vectorizer = HashingVectorizer(stop_words='english', non_negative=True, n_features = 2000)\n",
    "tfidf_transformer = TfidfTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_counts = count_vect.fit_transform(train_df['Anootated tweet'])\n",
    "X_test_counts = count_vect.transform(test_df['Anootated tweet'])\n",
    "\n",
    "# X_train_counts = hash_vectorizer.fit_transform(train_df['Anootated tweet'])\n",
    "# X_test_counts = hash_vectorizer.transform(test_df['Anootated tweet'])\n",
    "\n",
    "# X_train_counts = vectorizer.fit_transform(train_df['Anootated tweet'])\n",
    "# X_test_counts = vectorizer.transform(test_df['Anootated tweet'])\n",
    "\n",
    "X_train = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_test = tfidf_transformer.transform(X_test_counts)\n",
    "\n",
    "y_train = train_df['Class']\n",
    "y_test = test_df['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def trim(s):\n",
    "    \"\"\"Trim string to fit on terminal (assuming 80-column display)\"\"\"\n",
    "    return s if len(s) <= 80 else s[:77] + \"...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Classification function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def benchmark(clf):\n",
    "    print('_' * 80)\n",
    "    print(\"Training: \")\n",
    "    print(clf)\n",
    "    t0 = time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    train_time = time() - t0\n",
    "    print(\"train time: %0.3fs\" % train_time)\n",
    "\n",
    "    t0 = time()\n",
    "    pred = clf.predict(X_test)\n",
    "    test_time = time() - t0\n",
    "    print(\"test time:  %0.3fs\" % test_time)\n",
    "\n",
    "    score = metrics.accuracy_score(y_test, pred)\n",
    "    print(\"accuracy:   %0.3f\" % score)\n",
    "\n",
    "    if hasattr(clf, 'coef_'):\n",
    "        print(\"dimensionality: %d\" % clf.coef_.shape[1])\n",
    "        print(\"density: %f\" % density(clf.coef_))\n",
    "\n",
    "#         if feature_names is not None:\n",
    "#             print(\"top 10 keywords per class:\")\n",
    "#             for i, category in enumerate(categories):\n",
    "#                 top10 = np.argsort(clf.coef_[i])[-10:]\n",
    "#                 print(trim(\"%s: %s\"\n",
    "#                       % (category, \" \".join(feature_names[top10]))))\n",
    "#         print()\n",
    "\n",
    "    print(\"classification report:\")\n",
    "    print(metrics.classification_report(y_test, pred,\n",
    "                                        target_names=categories))\n",
    "\n",
    "    print(\"confusion matrix:\")\n",
    "    print(metrics.confusion_matrix(y_test, pred))\n",
    "\n",
    "    print()\n",
    "    clf_descr = str(clf).split('(')[0]\n",
    "    return clf_descr, score, train_time, test_time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Naive Bayes\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n",
      "train time: 0.021s\n",
      "test time:  0.000s\n",
      "accuracy:   0.510\n",
      "dimensionality: 500\n",
      "density: 1.000000\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.48      0.56      0.52       666\n",
      "          0       0.50      0.41      0.45       707\n",
      "          1       0.56      0.57      0.56       682\n",
      "\n",
      "avg / total       0.51      0.51      0.51      2055\n",
      "\n",
      "confusion matrix:\n",
      "[[370 171 125]\n",
      " [231 289 187]\n",
      " [169 123 390]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "categories = np.unique(y_train.values)\n",
    "# Train sparse Naive Bayes classifiers\n",
    "print('=' * 80)\n",
    "print(\"Naive Bayes\")\n",
    "results.append(benchmark(MultinomialNB(alpha=.01)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-Nearest Neighbour Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "kNN\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=-1, n_neighbors=10, p=2,\n",
      "           weights='uniform')\n",
      "train time: 0.008s\n",
      "test time:  1.185s\n",
      "accuracy:   0.452\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.46      0.29      0.36       666\n",
      "          0       0.41      0.60      0.49       707\n",
      "          1       0.53      0.45      0.48       682\n",
      "\n",
      "avg / total       0.46      0.45      0.44      2055\n",
      "\n",
      "confusion matrix:\n",
      "[[195 341 130]\n",
      " [133 427 147]\n",
      " [ 93 282 307]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train k-Nearest Neighbour classifiers\n",
    "print('=' * 80)\n",
    "print(\"kNN\")\n",
    "results.append(benchmark(KNeighborsClassifier(n_neighbors=10, n_jobs = -1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Random Forest\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "train time: 3.532s\n",
      "test time:  0.107s\n",
      "accuracy:   0.504\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.48      0.50      0.49       666\n",
      "          0       0.46      0.51      0.49       707\n",
      "          1       0.59      0.50      0.54       682\n",
      "\n",
      "avg / total       0.51      0.50      0.50      2055\n",
      "\n",
      "confusion matrix:\n",
      "[[331 233 102]\n",
      " [208 362 137]\n",
      " [154 186 342]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train Random Forest classifiers\n",
    "print('=' * 80)\n",
    "print(\"Random Forest\")\n",
    "results.append(benchmark(RandomForestClassifier(n_estimators=100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SGD Model\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
      "       learning_rate='optimal', loss='hinge', n_iter=50, n_jobs=1,\n",
      "       penalty='l1', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "train time: 0.175s\n",
      "test time:  0.000s\n",
      "accuracy:   0.524\n",
      "dimensionality: 500\n",
      "density: 0.750000\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.50      0.54      0.52       666\n",
      "          0       0.52      0.47      0.49       707\n",
      "          1       0.56      0.57      0.56       682\n",
      "\n",
      "avg / total       0.52      0.52      0.52      2055\n",
      "\n",
      "confusion matrix:\n",
      "[[362 164 140]\n",
      " [213 329 165]\n",
      " [156 140 386]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train SGD Classifier\n",
    "print('=' * 80)\n",
    "print(\"SGD Model\")\n",
    "results.append(benchmark(SGDClassifier(alpha=.0001, n_iter=50,\n",
    "                                           penalty='l1')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Linear SVM Model\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='l2', max_iter=1000, multi_class='ovr',\n",
      "     penalty='l2', random_state=None, tol=0.001, verbose=0)\n",
      "train time: 0.061s\n",
      "test time:  0.000s\n",
      "accuracy:   0.522\n",
      "dimensionality: 500\n",
      "density: 1.000000\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.49      0.52      0.50       666\n",
      "          0       0.50      0.49      0.49       707\n",
      "          1       0.58      0.56      0.57       682\n",
      "\n",
      "avg / total       0.52      0.52      0.52      2055\n",
      "\n",
      "confusion matrix:\n",
      "[[345 195 126]\n",
      " [214 344 149]\n",
      " [150 149 383]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kushagrathapar/anaconda/lib/python3.5/site-packages/sklearn/svm/classes.py:197: DeprecationWarning: loss='l2' has been deprecated in favor of loss='squared_hinge' as of 0.16. Backward compatibility for the loss='l2' will be removed in 1.0\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Train Linear SVM Classifier\n",
    "print('=' * 80)\n",
    "print(\"Linear SVM Model\")\n",
    "results.append(benchmark(LinearSVC(loss='l2', penalty='l2',\n",
    "                                            dual=False, tol=1e-3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Cross Validation (10 fold) on Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kushagrathapar/anaconda/lib/python3.5/site-packages/sklearn/svm/classes.py:197: DeprecationWarning: loss='l2' has been deprecated in favor of loss='squared_hinge' as of 0.16. Backward compatibility for the loss='l2' will be removed in 1.0\n",
      "  DeprecationWarning)\n",
      "/Users/kushagrathapar/anaconda/lib/python3.5/site-packages/sklearn/svm/classes.py:197: DeprecationWarning: loss='l2' has been deprecated in favor of loss='squared_hinge' as of 0.16. Backward compatibility for the loss='l2' will be removed in 1.0\n",
      "  DeprecationWarning)\n",
      "/Users/kushagrathapar/anaconda/lib/python3.5/site-packages/sklearn/svm/classes.py:197: DeprecationWarning: loss='l2' has been deprecated in favor of loss='squared_hinge' as of 0.16. Backward compatibility for the loss='l2' will be removed in 1.0\n",
      "  DeprecationWarning)\n",
      "/Users/kushagrathapar/anaconda/lib/python3.5/site-packages/sklearn/svm/classes.py:197: DeprecationWarning: loss='l2' has been deprecated in favor of loss='squared_hinge' as of 0.16. Backward compatibility for the loss='l2' will be removed in 1.0\n",
      "  DeprecationWarning)\n",
      "/Users/kushagrathapar/anaconda/lib/python3.5/site-packages/sklearn/svm/classes.py:197: DeprecationWarning: loss='l2' has been deprecated in favor of loss='squared_hinge' as of 0.16. Backward compatibility for the loss='l2' will be removed in 1.0\n",
      "  DeprecationWarning)\n",
      "/Users/kushagrathapar/anaconda/lib/python3.5/site-packages/sklearn/svm/classes.py:197: DeprecationWarning: loss='l2' has been deprecated in favor of loss='squared_hinge' as of 0.16. Backward compatibility for the loss='l2' will be removed in 1.0\n",
      "  DeprecationWarning)\n",
      "/Users/kushagrathapar/anaconda/lib/python3.5/site-packages/sklearn/svm/classes.py:197: DeprecationWarning: loss='l2' has been deprecated in favor of loss='squared_hinge' as of 0.16. Backward compatibility for the loss='l2' will be removed in 1.0\n",
      "  DeprecationWarning)\n",
      "/Users/kushagrathapar/anaconda/lib/python3.5/site-packages/sklearn/svm/classes.py:197: DeprecationWarning: loss='l2' has been deprecated in favor of loss='squared_hinge' as of 0.16. Backward compatibility for the loss='l2' will be removed in 1.0\n",
      "  DeprecationWarning)\n",
      "/Users/kushagrathapar/anaconda/lib/python3.5/site-packages/sklearn/svm/classes.py:197: DeprecationWarning: loss='l2' has been deprecated in favor of loss='squared_hinge' as of 0.16. Backward compatibility for the loss='l2' will be removed in 1.0\n",
      "  DeprecationWarning)\n",
      "/Users/kushagrathapar/anaconda/lib/python3.5/site-packages/sklearn/svm/classes.py:197: DeprecationWarning: loss='l2' has been deprecated in favor of loss='squared_hinge' as of 0.16. Backward compatibility for the loss='l2' will be removed in 1.0\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.53983605  0.56204608  0.53196383  0.54927391  0.52771317  0.54905167\n",
      "  0.52128698  0.51588434  0.54614218  0.50499276]\n",
      "Accuracy: 0.53 (+/- 0.03)\n"
     ]
    }
   ],
   "source": [
    "# print('The scikit-learn version is {}.'.format(sklearn.__version__))\n",
    "\n",
    "tweet_random_df = tweet_df.copy()\n",
    "len(tweet_random_df)\n",
    "for i in range(0, 100):\n",
    "    split1_df, split2_df = splitTrainingData(tweet_random_df, 50)\n",
    "    tweet_random_df = pd.concat([split1_df, split2_df])\n",
    "    \n",
    "X_kfcv_counts = count_vect.fit_transform(tweet_random_df['Anootated tweet'])\n",
    "X_kfcv = tfidf_transformer.fit_transform(X_kfcv_counts)\n",
    "Y_kfcv = tweet_random_df['Class']\n",
    "\n",
    "clf = LinearSVC(loss='l2', penalty='l2',dual=False, tol=1e-3)\n",
    "scores = cross_validation.cross_val_score(clf, X_kfcv, Y_kfcv, cv=10, scoring='f1_weighted')\n",
    "print(scores)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Cross Validation (10 fold) on Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.53348124  0.52001223  0.52535702  0.52158433  0.53633513  0.55741822\n",
      "  0.52025087  0.51150223  0.53409707  0.5227049 ]\n",
      "Accuracy: 0.53 (+/- 0.02)\n"
     ]
    }
   ],
   "source": [
    "clf = MultinomialNB(alpha = 0.01)\n",
    "scores = cross_validation.cross_val_score(clf, X_kfcv, Y_kfcv, cv=10, scoring='f1_weighted')\n",
    "print(scores)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Cross Validation (10 fold) on Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.54115181  0.52506213  0.53533824  0.53426686  0.52782431  0.54960863\n",
      "  0.50247266  0.51693942  0.54068965  0.49022367]\n",
      "Accuracy: 0.53 (+/- 0.04)\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "scores = cross_validation.cross_val_score(clf, X_kfcv, Y_kfcv, cv=10, scoring='f1_weighted')\n",
    "print(scores)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation (10 fold) on Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.52181847  0.53596984  0.51449764  0.51909145  0.52562748  0.52783346\n",
      "  0.51326118  0.50353267  0.53520147  0.50669567]\n",
      "Accuracy: 0.52 (+/- 0.02)\n"
     ]
    }
   ],
   "source": [
    "clf = SGDClassifier(alpha=.0001, n_iter=50, penalty='l1')\n",
    "scores = cross_validation.cross_val_score(clf, X_kfcv, Y_kfcv, cv=10, scoring='f1_weighted')\n",
    "print(scores)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=10, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KNeighborsClassifier(n_neighbors=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
